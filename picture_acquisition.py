# -*- coding: utf-8 -*-
"""picture_acquisition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1afQ6FcowCgarTuhDGI5ZbvhoJG5ge5C1
"""

!pip install cinemagoer
!pip install selenium
!pip install undetected_chromedriver
!pip install serpapi

import os
import torch
import numpy as np
import torchvision.transforms as transforms
from torchvision import models
from sklearn.cluster import KMeans
from PIL import Image
import requests
from google.colab import userdata

SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')

# Function to download an image from URL
def download_image(url, save_path):
    """Download an image from url to save_path."""
    try:
        response = requests.get(url, stream=True, timeout=10)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except Exception as e:
        print(f"Error downloading {url}: {e}")
        return False

# SerpAPI image download function
try:
    # Try the updated API (newer versions)
    import serpapi

    def download_serpapi_images(query, dest_folder, max_images=5):
        """Use SerpAPI to fetch up to max_images stills for `query`."""
        os.makedirs(dest_folder, exist_ok=True)
        params = {
            "engine": "google_images",
            "q": query,
            "api_key": SERPAPI_API_KEY,
            "num": max_images,
        }
        results = serpapi.search(params)
        images = results.get("images_results", [])[:max_images]
        for idx, img in enumerate(images, start=1):
            url = img.get("original") or img.get("thumbnail")
            if not url:
                continue
            fname = os.path.join(dest_folder, f"serpapi_{idx}.jpg")
            download_image(url, fname)

except (ImportError, AttributeError):
    # Try the classic API (older versions)
    # First make sure we have the correct package
    try:
        from serpapi import GoogleSearch
    except ImportError:
        # Install the correct package if it's not available
        !pip install google-search-results
        from serpapi import GoogleSearch

    def download_serpapi_images(query, dest_folder, max_images=5):
        """Use SerpAPI to fetch up to max_images stills for `query`."""
        os.makedirs(dest_folder, exist_ok=True)
        params = {
            "engine": "google_images",
            "q": query,
            "api_key": SERPAPI_API_KEY,
            "num": max_images,
        }
        search = GoogleSearch(params)
        results = search.get_dict()
        images = results.get("images_results", [])[:max_images]
        for idx, img in enumerate(images, start=1):
            url = img.get("original") or img.get("thumbnail")
            if not url:
                continue
            fname = os.path.join(dest_folder, f"serpapi_{idx}.jpg")
            download_image(url, fname)

def select_representative_images(image_paths, top_n=5, device="cpu"):
    """Extract CNN features for each path and pick top_n diverse/representative images by K-means."""
    # 1) load a pretrained ResNet50 up to penultimate layer
    model = models.resnet50(pretrained=True)
    model = torch.nn.Sequential(*list(model.children())[:-1]).to(device).eval()

    # 2) preprocessing
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])

    feats = []
    valid_paths = []
    for p in image_paths:
        try:
            img = Image.open(p).convert("RGB")
            tensor = preprocess(img).unsqueeze(0).to(device)
            with torch.no_grad():
                out = model(tensor)      # shape (1, 2048, 1, 1)
            vec = out.squeeze().cpu().numpy()  # (2048,)
            feats.append(vec)
            valid_paths.append(p)
        except Exception as e:
            print(f"  - failed to extract feat from {p}: {e}")

    if len(feats) <= top_n:
        return valid_paths

    # 3) K-means cluster into top_n clusters, pick closest-to-centroid per cluster
    X = np.stack(feats, axis=0)
    km = KMeans(n_clusters=top_n, random_state=0).fit(X)
    centers = km.cluster_centers_
    labels = km.labels_

    selected = []
    for i in range(top_n):
        idxs = np.where(labels == i)[0]
        # distances to centroid
        dists = np.linalg.norm(X[idxs] - centers[i], axis=1)
        best = idxs[np.argmin(dists)]
        selected.append(valid_paths[best])

    return selected

# --- EXAMPLE USAGE INSIDE process_movie_by_id() ---
def process_movie_by_id(movie_id, rank, imdb_features):
    """Process a movie by its ID, download images, and select representative ones."""
    # Create a directory for the movie
    movie_dir = f"movie_{movie_id}"
    os.makedirs(movie_dir, exist_ok=True)

    # 1) Download SerpAPI stills
    serpapi_folder = os.path.join(movie_dir, "serpapi")
    download_serpapi_images(f"{imdb_features['Title']} movie stills", serpapi_folder, max_images=5)

    # 2) Gather all downloaded image paths
    all_imgs = []
    for fname in os.listdir(movie_dir):
        if fname.lower().endswith(".jpg"):
            all_imgs.append(os.path.join(movie_dir, fname))

    # Also include serpapi subfolder
    if os.path.exists(serpapi_folder):
        for fname in os.listdir(serpapi_folder):
            if fname.lower().endswith(".jpg"):
                all_imgs.append(os.path.join(serpapi_folder, fname))

    # 3) Pick the 5 most representative images
    selected = select_representative_images(all_imgs, top_n=5)
    print(f"Selected representative images:\n  " + "\n  ".join(selected))

    # Return the selected images or do further processing
    return selected

# Example of how to set up the API key in Colab (run this once)
# You'll be prompted to input your key which will be securely stored
# This is a commented example - uncomment and run separately if needed
"""
from google.colab import userdata

# This will open a dialog to input your API key
# The key will be stored securely and won't appear in your notebook
# Access it with userdata.get('SERPAPI_API_KEY')
userdata.set('SERPAPI_API_KEY', 'your-key-here')
"""

# Example usage:
if __name__ == "__main__":
    # This is a dummy example - replace with your actual code
    movie_info = {"Title": "The Godfather"}
    selected_images = process_movie_by_id("tt0068646", 1, movie_info)
    print(f"Final selected images: {selected_images}")

import os
import requests
import time
import json
import traceback
from PIL import Image
from io import BytesIO
from tqdm import tqdm
from google.colab import userdata

# Install necessary packages
try:
    from serpapi import GoogleSearch
except ImportError:
    !pip install google-search-results
    from serpapi import GoogleSearch

try:
    from imdb import Cinemagoer
except ImportError:
    !pip install cinemagoer
    from imdb import Cinemagoer

# Securely get API keys from Colab
SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')
TMDB_API_KEY = userdata.get('TMDB_API_KEY')

# IMDb top 10 movies list with their IMDb IDs
TOP_10_IMDB_IDS = [
    "0111161",  # 1. The Shawshank Redemption
    "0068646",  # 2. The Godfather
    "0071562",  # 3. The Godfather: Part II
    "0468569",  # 4. The Dark Knight
    "0050083",  # 5. 12 Angry Men
    "0108052",  # 6. Schindler's List
    "0167260",  # 7. The Lord of the Rings: The Return of the King
    "0110912",  # 8. Pulp Fiction
    "0060196",  # 9. The Good, the Bad and the Ugly
    "0120737",  # 10. The Lord of the Rings: The Fellowship of the Ring
]

print("Initializing Cinemagoer...")
ia = Cinemagoer()

def clean_filename(name):
    """Clean filename to remove invalid characters"""
    invalid = '<>:"/\\|?*'
    for c in invalid:
        name = name.replace(c, '_')
    return name

def download_image(url, save_path, headers=None):
    """Download an image from URL to save_path"""
    if os.path.exists(save_path):
        print(f"  - Image already exists: {save_path}")
        return True

    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        with open(save_path, 'wb') as f:
            f.write(response.content)

        # Optionally optimize the image
        try:
            img = Image.open(BytesIO(response.content))
            if img.mode != 'RGB':
                img = img.convert('RGB')

            # Resize if too large
            max_dim = 1200
            w, h = img.size
            if w > max_dim or h > max_dim:
                if w > h:
                    new_w = max_dim
                    new_h = int(h * (max_dim / w))
                else:
                    new_h = max_dim
                    new_w = int(w * (max_dim / h))
                img = img.resize((new_w, new_h), Image.LANCZOS)

            img.save(save_path, 'JPEG', quality=95, optimize=True)
        except Exception as e:
            print(f"  - Error optimizing image: {e}")

        print(f"  - Downloaded: {save_path}")
        return True

    except Exception as e:
        print(f"  - Failed to download {url}: {e}")
        return False

def download_serpapi_images(query, dest_folder, max_images=5):
    """Use SerpAPI to fetch movie stills"""
    os.makedirs(dest_folder, exist_ok=True)

    params = {
        "engine": "google_images",
        "q": query,
        "api_key": SERPAPI_API_KEY,
        "num": max_images * 2,  # Get more to ensure we have enough valid ones
    }

    print(f"Searching for images: '{query}'")
    search = GoogleSearch(params)
    results = search.get_dict()
    images = results.get("images_results", [])

    successful_downloads = 0
    for i, img in enumerate(images):
        if successful_downloads >= max_images:
            break

        url = img.get("original") or img.get("thumbnail")
        if not url:
            continue

        file_path = os.path.join(dest_folder, f"backdrop_serpapi_{successful_downloads+1}.jpg")

        if download_image(url, file_path):
            successful_downloads += 1

        # Avoid rate limiting
        time.sleep(0.5)

    print(f"Downloaded {successful_downloads} images from SerpAPI")
    return successful_downloads

def get_imdb_movie_details(imdb_id):
    """Get movie details from IMDb using Cinemagoer"""
    if not imdb_id.startswith('tt'):
        imdb_id = f"tt{imdb_id}"

    print(f"Fetching IMDb details for {imdb_id}...")
    try:
        movie = ia.get_movie(imdb_id[2:])  # Remove 'tt' prefix for Cinemagoer
        return movie
    except Exception as e:
        print(f"Error fetching IMDb details: {e}")
        return None

def extract_imdb_features(movie, rank):
    """Extract key features from IMDb movie object"""
    if not movie:
        return {}

    features = {
        "Rank": rank,
        "Title": movie.get('title', ''),
        "Year": movie.get('year', ''),
        "IMDb_ID": movie.get('imdbID', ''),
        "Rating": movie.get('rating', ''),
        "Genres": '|'.join(movie.get('genres', [])),
        "Directors": '|'.join([d.get('name', '') for d in movie.get('directors', []) if 'name' in d]),
        "Cast": '|'.join([a.get('name', '') for a in movie.get('cast', [])[:5] if 'name' in a]),
        "Plot": movie.get('plot outline', '')
    }
    features["Poster_URL"] = movie.get('full-size cover url', movie.get('cover url', ''))
    return features

def find_tmdb_id_by_imdb_id(imdb_id):
    """Find TMDB ID from IMDb ID"""
    if not imdb_id.startswith('tt'):
        imdb_id = f"tt{imdb_id}"

    url = f"https://api.themoviedb.org/3/find/{imdb_id}"
    params = {"api_key": TMDB_API_KEY, "external_source": "imdb_id"}

    try:
        response = requests.get(url, params=params)
        if response.status_code == 200:
            data = response.json()
            if data["movie_results"]:
                return data["movie_results"][0]["id"]
            print(f"No TMDB movie found for IMDb ID {imdb_id}")
        else:
            print(f"TMDB API request failed: {response.status_code}")
    except Exception as e:
        print(f"Error finding TMDB ID: {e}")

    return None

def get_tmdb_movie_images(tmdb_id):
    """Get backdrops from TMDB"""
    url = f"https://api.themoviedb.org/3/movie/{tmdb_id}/images"
    params = {"api_key": TMDB_API_KEY}

    try:
        response = requests.get(url, params=params)
        if response.status_code == 200:
            data = response.json()
            return data.get("backdrops", [])
        else:
            print(f"Failed to get TMDB images: {response.status_code}")
    except Exception as e:
        print(f"Error getting TMDB images: {e}")

    return []

def download_tmdb_backdrops(backdrops, movie_dir, max_count=5):
    """Download backdrop images from TMDB"""
    downloaded = 0

    for i, backdrop in enumerate(backdrops[:max_count]):
        file_path = backdrop.get("file_path")
        if not file_path:
            continue

        img_url = f"https://image.tmdb.org/t/p/original{file_path}"
        save_path = os.path.join(movie_dir, f"backdrop_{i+1}.jpg")

        if download_image(img_url, save_path):
            downloaded += 1

        time.sleep(0.2)  # Small delay between downloads

    print(f"Downloaded {downloaded} backdrops from TMDB")
    return downloaded

def download_imdb_poster(poster_url, movie_dir):
    """Download IMDb poster"""
    if not poster_url:
        print("No poster URL provided")
        return False

    save_path = os.path.join(movie_dir, "imdb_poster.jpg")
    return download_image(poster_url, save_path)

def process_movie(imdb_id, rank):
    """Process a single movie by IMDb ID"""
    print(f"\n{'='*50}")
    print(f"Processing #{rank}: IMDb ID tt{imdb_id}")
    print(f"{'='*50}")

    # Create base directory for this movie
    movie_dir = os.path.join("imdb_top_10", f"{rank:02d}_tt{imdb_id}")
    os.makedirs(movie_dir, exist_ok=True)

    # Get IMDb details
    imdb_movie = get_imdb_movie_details(imdb_id)
    if not imdb_movie:
        print(f"Could not retrieve IMDb data for tt{imdb_id}, skipping")
        return False

    movie_title = imdb_movie.get('title', f"Movie_{imdb_id}")
    print(f"Movie: {movie_title} ({imdb_movie.get('year', 'Unknown')})")

    # Extract relevant IMDb features
    imdb_features = extract_imdb_features(imdb_movie, rank)

    # Download IMDb poster
    poster_url = imdb_features.get("Poster_URL")
    if poster_url:
        print("Downloading IMDb poster...")
        download_imdb_poster(poster_url, movie_dir)
    else:
        print("No IMDb poster URL found")

    # Get TMDB ID
    tmdb_id = find_tmdb_id_by_imdb_id(imdb_id)
    if not tmdb_id:
        print(f"Could not find TMDB ID for IMDb ID tt{imdb_id}")
    else:
        print(f"Found TMDB ID: {tmdb_id}")

        # Get and download TMDB backdrops
        backdrops = get_tmdb_movie_images(tmdb_id)
        if backdrops:
            print(f"Found {len(backdrops)} backdrops on TMDB")
            download_tmdb_backdrops(backdrops, movie_dir)
        else:
            print("No TMDB backdrops found")

    # Download stills using SerpAPI
    print("Downloading movie stills using SerpAPI...")
    download_serpapi_images(f"{movie_title} movie stills", movie_dir)

    print(f"Finished processing #{rank}: {movie_title}")
    return True

def main():
    """Process all the top 10 IMDb movies"""
    print("Starting IMDb Top 10 Movies Processing")

    # Ensure base directory exists
    os.makedirs("imdb_top_10", exist_ok=True)

    # Process each movie
    for rank, imdb_id in enumerate(TOP_10_IMDB_IDS, 1):
        process_movie(imdb_id, rank)
        # Add a delay between movies to avoid API rate limits
        if rank < len(TOP_10_IMDB_IDS):
            print("Waiting before processing next movie...")
            time.sleep(2)

    print("\nAll movies processed!")
    print("Files are saved in the 'imdb_top_10' directory")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"An error occurred: {e}")
        traceback.print_exc()