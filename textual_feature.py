# -*- coding: utf-8 -*-
"""textual_feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SxvSTHewX_gfMm2YTuEKLQXYAdB-5Tun
"""

import os
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel
import requests
import time
from tqdm import tqdm
import re
import json
import warnings

class TextualFeatureExtractor:
    """
    Extract and concatenate textual features from plot, trailer, and tagline
    Inputs: enriched plot (GROK), trailer summary & quotes, tagline explanation (GROK)
    Encoder: BERT → 768-D each for plot / trailer / tagline
    Concatenation: [plot, trailer, tagline] → 2304-D textual vector
    """

    def __init__(self, data_dir='.'):
        """Initialize the text feature extractor"""
        print("Initializing text feature extractor...")

        self.data_dir = data_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Load BERT model and tokenizer
        print("Loading BERT model and tokenizer...")
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained('bert-base-uncased').to(self.device)
        self.model.eval()

        # GROK API settings - you would need to provide your own API key
        self.use_grok = False
        try:
            # Try to load API key from environment or config file
            self.grok_api_key = os.environ.get('GROK_API_KEY', '')
            if self.grok_api_key:
                self.use_grok = True
                print("GROK API key loaded successfully.")
            else:
                print("GROK API key not found. Will skip GROK enrichment.")
        except:
            print("Failed to configure GROK API. Will skip GROK enrichment.")

    def load_movie_data(self):
        """Load movie data including IMDb plot, taglines, and trailer summaries"""
        print("Loading movie data...")

        # Try to load IMDb/movie data
        try:
            # This assumes you have a CSV with IMDb movie data
            movies_path = os.path.join(self.data_dir, "top_movies.csv")
            imdb_df = pd.read_csv(movies_path)
            print(f"Loaded {len(imdb_df)} movies from IMDb dataset")
        except Exception as e:
            print(f"Error loading movie data: {e}")
            print("Creating empty DataFrame")
            imdb_df = pd.DataFrame(columns=['IMDb_ID', 'Title', 'Plot', 'Taglines'])

        # Try to load trailer summaries
        try:
            trailer_path = os.path.join(self.data_dir, "trailer_summary.csv")
            trailer_df = pd.read_csv(trailer_path)
            print(f"Loaded {len(trailer_df)} trailer summaries")

            # The first column is IMDb ID without 'tt'
            # Add 'tt' prefix if needed
            id_col = trailer_df.columns[0]
            if not trailer_df[id_col].astype(str).str.startswith('tt').all():
                trailer_df['IMDb_ID'] = 'tt' + trailer_df[id_col].astype(str).str.zfill(7)
            else:
                trailer_df['IMDb_ID'] = trailer_df[id_col]

            # Assuming trailer summary is in the second column
            trailer_df['Trailer_Summary'] = trailer_df.iloc[:, 1]

            # Merge trailer summaries with movie data
            imdb_df = pd.merge(imdb_df, trailer_df[['IMDb_ID', 'Trailer_Summary']],
                              on='IMDb_ID', how='left')

            print(f"Merged {imdb_df['Trailer_Summary'].notna().sum()} trailer summaries with movie data")
        except Exception as e:
            print(f"Error loading trailer summaries: {e}")
            imdb_df['Trailer_Summary'] = None

        return imdb_df

    def enrich_plot_with_grok(self, title, plot):
        """Use GROK API to enrich short plot descriptions"""
        if not self.use_grok:
            return plot

        # Check if the plot is too short (less than 30 words)
        if plot and len(plot.split()) >= 30:
            return plot

        # Prepare prompt for GROK - directly in English
        prompt = f"Please provide a plot summary for the movie '{title}', about 50 words. Include background, style, and cinematography if relevant."

        try:
            # Make API call to GROK
            headers = {
                "Authorization": f"Bearer {self.grok_api_key}",
                "Content-Type": "application/json"
            }

            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": "grok-1"  # Specify the model
            }

            response = requests.post(
                "https://api.grok.ai/v1/chat/completions",
                headers=headers,
                json=data
            )

            if response.status_code == 200:
                result = response.json()
                enriched_plot = result['choices'][0]['message']['content']
                print(f"Successfully enriched plot for '{title}'")
                return enriched_plot
            else:
                print(f"GROK API error: {response.status_code}, {response.text}")
                return plot
        except Exception as e:
            print(f"Error calling GROK API: {e}")
            return plot

    def explain_tagline_with_grok(self, title, tagline):
        """Use GROK API to explain the significance of a movie tagline"""
        if not self.use_grok or not tagline:
            return ""

        # Prepare prompt for GROK - directly in English
        prompt = f"Please explain why the tagline '{tagline}' has a unique significance for the movie '{title}'."

        try:
            # Make API call to GROK
            headers = {
                "Authorization": f"Bearer {self.grok_api_key}",
                "Content-Type": "application/json"
            }

            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": "grok-1"  # Specify the model
            }

            response = requests.post(
                "https://api.grok.ai/v1/chat/completions",
                headers=headers,
                json=data
            )

            if response.status_code == 200:
                result = response.json()
                tagline_explanation = result['choices'][0]['message']['content']
                print(f"Successfully got tagline explanation for '{title}'")
                return tagline_explanation
            else:
                print(f"GROK API error: {response.status_code}, {response.text}")
                return ""
        except Exception as e:
            print(f"Error calling GROK API: {e}")
            return ""

    def extract_bert_features(self, text):
        """Extract BERT features from text"""
        if not text or pd.isna(text):
            # Return zeros for empty text
            return np.zeros(768)

        try:
            # Tokenize and encode the text
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )

            # Move inputs to device
            inputs = {key: val.to(self.device) for key, val in inputs.items()}

            # Get BERT embeddings
            with torch.no_grad():
                outputs = self.model(**inputs)

                # Use [CLS] token embedding as the feature vector
                features = outputs.last_hidden_state[:, 0, :].cpu().numpy()

            return features.flatten()
        except Exception as e:
            print(f"Error extracting BERT features: {e}")
            return np.zeros(768)

    def process_movie_textual_features(self, movies_df):
        """
        Process all textual features for movies

        Args:
            movies_df: DataFrame with movie information including plot, taglines, etc.

        Returns:
            DataFrame with movie IDs and 2304-D textual feature vectors
        """
        print("\nProcessing textual features for movies...")

        # Check if IMDb_ID column exists
        if 'IMDb_ID' not in movies_df.columns:
            print("Error: DataFrame must have 'IMDb_ID' column")
            return None

        # Prepare result container
        results = []

        # Get required columns with defaults
        id_col = 'IMDb_ID'
        title_col = 'Title' if 'Title' in movies_df.columns else 'title'
        plot_col = 'Plot' if 'Plot' in movies_df.columns else 'Plot_Outline' if 'Plot_Outline' in movies_df.columns else 'plot'
        tagline_col = 'Taglines' if 'Taglines' in movies_df.columns else 'taglines' if 'taglines' in movies_df.columns else None
        trailer_col = 'Trailer_Summary' if 'Trailer_Summary' in movies_df.columns else None

        # Process each movie
        for idx, row in tqdm(movies_df.iterrows(), total=len(movies_df), desc="Extracting textual features"):
            try:
                movie_id = row[id_col]
                title = row[title_col] if title_col in row and not pd.isna(row[title_col]) else "Unknown Title"

                # Get plot text
                plot = row[plot_col] if plot_col in row and not pd.isna(row[plot_col]) else ""

                # Enrich short plots with GROK
                if self.use_grok and plot and len(str(plot).split()) < 30:
                    plot = self.enrich_plot_with_grok(title, str(plot))

                # Get tagline
                tagline = row[tagline_col] if tagline_col in row and not pd.isna(row[tagline_col]) else ""

                # For pipe-separated taglines, take the first one
                if isinstance(tagline, str) and '|' in tagline:
                    tagline = tagline.split('|')[0].strip()

                # Get tagline explanation from GROK
                tagline_explanation = ""
                if self.use_grok and tagline:
                    tagline_explanation = self.explain_tagline_with_grok(title, tagline)
                else:
                    # If GROK is not available, use the tagline itself
                    tagline_explanation = tagline

                # Get trailer summary
                trailer_summary = row[trailer_col] if trailer_col in row and not pd.isna(row[trailer_col]) else ""

                # Extract BERT features for each text component
                plot_features = self.extract_bert_features(plot)
                trailer_features = self.extract_bert_features(trailer_summary)
                tagline_features = self.extract_bert_features(tagline_explanation)

                # Concatenate all features
                all_features = np.concatenate([plot_features, trailer_features, tagline_features])

                # Store results
                result = {'movie_id': movie_id}

                # Add each feature dimension
                for i, feat in enumerate(all_features):
                    result[f'text_feature_{i+1}'] = feat

                results.append(result)

            except Exception as e:
                print(f"Error processing movie {row.get(id_col, 'unknown')}: {e}")

        # Create DataFrame from results
        if results:
            textual_features_df = pd.DataFrame(results)
            return textual_features_df
        else:
            print("No textual features were extracted")
            return None

    def save_features(self, features_df, output_file='textual_features.csv'):
        """Save extracted features to CSV file"""
        if features_df is None or features_df.empty:
            print("No features to save")
            return False

        try:
            output_path = os.path.join(self.data_dir, output_file)
            features_df.to_csv(output_path, index=False)
            print(f"Saved {len(features_df)} textual feature vectors to {output_path}")
            return True
        except Exception as e:
            print(f"Error saving features: {e}")
            return False

    def extract_and_save_textual_features(self):
        """Main method to extract and save all textual features"""
        print("=== Plot & Trailer Text + Tagline Feature Extraction ===")

        # Load movie data
        movies_df = self.load_movie_data()

        if movies_df.empty:
            print("No movie data to process")
            return None

        # Process textual features
        textual_features = self.process_movie_textual_features(movies_df)

        # Save features
        if textual_features is not None:
            self.save_features(textual_features, "textual_features.csv")

            # Return the features
            return textual_features
        else:
            return None


def main():
    """Main function to run the textual feature extraction"""
    print("=== Textual Feature Extraction Pipeline ===")

    # Check for GPU availability
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Initialize and run the extractor
    extractor = TextualFeatureExtractor(data_dir='.')
    textual_features = extractor.extract_and_save_textual_features()

    if textual_features is not None:
        # Display sample of the textual features
        feature_cols = [col for col in textual_features.columns if 'text_feature_' in col]
        sample_cols = ['movie_id'] + feature_cols[:5]  # Show just first 5 dimensions

        print("\nSample of extracted textual features (first 5 dimensions):")
        print(textual_features[sample_cols].head())

        print(f"\nTotal dimensions: {len(feature_cols)} (768-D plot + 768-D trailer + 768-D tagline)")
    else:
        print("Failed to extract textual features")


if __name__ == "__main__":
    main()