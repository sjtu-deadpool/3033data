# -*- coding: utf-8 -*-
"""merge_data_250filterVersion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1afQ6FcowCgarTuhDGI5ZbvhoJG5ge5C1
"""

!pip install cinemagoer
!pip install selenium
!pip install undetected_chromedriver
!pip install serpapi

import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler
from torch import nn, optim
import torch
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

class StructuredDataProcessor:
    """
    Process structured metadata for movies and users according to requirements:
    - Movie fields: title, year, genres, runtime, etc. (excluding genome tags)
    - User fields: gender, age, occupation
    - Preprocessing: one-hot/label encoding + normalization
    - MLP projection: 128-D movie & 32-D user vectors
    """

    def __init__(self, data_dir="../"):
        """Initialize with path to MovieLens data"""
        self.data_dir = data_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Model hyperparameters
        self.movie_embedding_dim = 128
        self.user_embedding_dim = 32
        self.batch_size = 64
        self.epochs = 50
        self.learning_rate = 1e-3

    def load_data(self):
        """Load MovieLens data files"""
        print("Loading data files...")

        try:
            # Load movies.csv
            movies_path = os.path.join(self.data_dir, "movies.csv")
            self.movies_df = pd.read_csv(movies_path)
            print(f"Loaded {len(self.movies_df)} movies")

            # Load ratings.csv for aggregated statistics
            ratings_path = os.path.join(self.data_dir, "ratings.csv")
            self.ratings_df = pd.read_csv(ratings_path)
            print(f"Loaded {len(self.ratings_df)} ratings")

            # Try to load links.csv for connecting to external IDs
            try:
                links_path = os.path.join(self.data_dir, "links.csv")
                self.links_df = pd.read_csv(links_path)
                print(f"Loaded {len(self.links_df)} links")
            except:
                print("links.csv not found, proceeding without external IDs")
                self.links_df = None

            # Try to load tags.csv (but exclude genome tags as specified)
            try:
                tags_path = os.path.join(self.data_dir, "tags.csv")
                self.tags_df = pd.read_csv(tags_path)
                print(f"Loaded {len(self.tags_df)} tags")
            except:
                print("tags.csv not found, proceeding without tags")
                self.tags_df = None

            # Try to load users.csv
            try:
                users_path = os.path.join(self.data_dir, "users.csv")
                self.users_df = pd.read_csv(users_path)
                print(f"Loaded {len(self.users_df)} users")
            except:
                print("users.csv not found, creating basic user data from ratings")
                # Create basic user DataFrame from ratings
                self.users_df = pd.DataFrame({"userId": self.ratings_df["userId"].unique()})
                print(f"Created basic data for {len(self.users_df)} users")

            return True

        except Exception as e:
            print(f"Error loading data: {e}")
            return False

    def extract_year_from_title(self):
        """Extract year from movie titles and clean title text"""
        print("Extracting years from movie titles...")

        # Extract year using regex (four digits inside parentheses at the end)
        self.movies_df["year"] = self.movies_df["title"].str.extract(r"$(\d{4})$").astype("float")

        # Remove year information from titles
        self.movies_df["clean_title"] = self.movies_df["title"].str.replace(r"\s*$\d{4}$\s*", "", regex=True)

        # Fill missing years with median
        median_year = self.movies_df["year"].median()
        self.movies_df["year"] = self.movies_df["year"].fillna(median_year)

        print(f"Extracted years ranging from {self.movies_df['year'].min():.0f} to {self.movies_df['year'].max():.0f}")

    def process_movie_features(self):
        """Process all movie features and create encoding"""
        print("Processing movie features...")

        # Extract year if not already done
        if "year" not in self.movies_df.columns:
            self.extract_year_from_title()

        # Create a copy to avoid modifying original
        movie_features = self.movies_df.copy()

        # --- Process genres with one-hot encoding ---
        if "genres" in movie_features.columns:
            # Split the pipe-separated genres
            all_genres = set()
            for genres in movie_features["genres"].dropna():
                if isinstance(genres, str) and "|" in genres:
                    all_genres.update(genres.split("|"))
                elif isinstance(genres, str):
                    all_genres.add(genres)

            # Remove '(no genres listed)' if present
            if "(no genres listed)" in all_genres:
                all_genres.remove("(no genres listed)")

            print(f"One-hot encoding {len(all_genres)} unique genres")

            # Create one-hot encoded columns for each genre
            for genre in all_genres:
                if genre:
                    movie_features[f"genre_{genre}"] = movie_features["genres"].apply(
                        lambda x: 1 if isinstance(x, str) and genre in x.split("|") else 0
                    )

        # --- Add movie statistics from ratings ---
        if self.ratings_df is not None and not self.ratings_df.empty:
            print("Adding movie statistics from ratings...")

            # Compute various statistics per movie
            rating_stats = self.ratings_df.groupby("movieId").agg({
                "rating": ["count", "mean", "std", "min", "max"]
            }).reset_index()

            # Flatten column names
            rating_stats.columns = [
                "movieId", "rating_count", "rating_mean",
                "rating_std", "rating_min", "rating_max"
            ]

            # Fill NaN in std (happens when movie has only one rating)
            rating_stats["rating_std"] = rating_stats["rating_std"].fillna(0)

            # Merge with movie features
            movie_features = pd.merge(movie_features, rating_stats, on="movieId", how="left")

            # Fill missing statistics with zeros
            for col in ["rating_count", "rating_mean", "rating_std", "rating_min", "rating_max"]:
                movie_features[col] = movie_features[col].fillna(0)

        # --- Process user tags (excluding genome tags) ---
        if self.tags_df is not None and not self.tags_df.empty:
            print("Processing user-provided tags...")

            # Count tag occurrences per movie
            tag_counts = self.tags_df.groupby(["movieId", "tag"]).size().reset_index(name="count")

            # Get top 5 tags per movie
            top_tags = tag_counts.sort_values(["movieId", "count"], ascending=[True, False])
            top_tags = top_tags.groupby("movieId").head(5)

            # Combine tags into a single string per movie
            movie_tags = top_tags.groupby("movieId")["tag"].apply(lambda x: "|".join(x)).reset_index()
            movie_tags.columns = ["movieId", "top_tags"]

            # Merge tags with movie features
            movie_features = pd.merge(movie_features, movie_tags, on="movieId", how="left")
            movie_features["top_tags"] = movie_features["top_tags"].fillna("")

            # One-hot encode the most common tags
            # First get counts of all tags
            all_tags = []
            for tags_str in movie_features["top_tags"].dropna():
                if tags_str:
                    all_tags.extend(tags_str.split("|"))

            # Count occurrences of each tag
            tag_counts = pd.Series(all_tags).value_counts()

            # Select top 50 tags for one-hot encoding to avoid too many features
            top_50_tags = tag_counts.head(50).index.tolist()

            print(f"One-hot encoding top {len(top_50_tags)} user tags")

            # Create one-hot features for top tags
            for tag in top_50_tags:
                movie_features[f"tag_{tag}"] = movie_features["top_tags"].apply(
                    lambda x: 1 if isinstance(x, str) and x and tag in x.split("|") else 0
                )

        # --- Add external data if merged in previous steps ---
        external_numeric_cols = [
            "Runtime_Minutes", "TMDB_Runtime", "TMDB_Vote_Average",
            "TMDB_Vote_Count", "TMDB_Popularity", "TMDB_Budget", "TMDB_Revenue"
        ]

        for col in external_numeric_cols:
            if col in movie_features.columns:
                # Convert to numeric, coercing errors to NaN
                movie_features[col] = pd.to_numeric(movie_features[col], errors="coerce")

                # Fill missing values with median
                median_val = movie_features[col].median()
                movie_features[col] = movie_features[col].fillna(median_val)

        # --- Select final features ---
        # Keep the ID column
        final_features = ["movieId"]

        # Add numeric columns
        numeric_cols = ["year", "rating_count", "rating_mean", "rating_std", "rating_min", "rating_max"]
        numeric_cols.extend([col for col in external_numeric_cols if col in movie_features.columns])
        final_features.extend(numeric_cols)

        # Add categorical (one-hot encoded) columns
        categorical_cols = [col for col in movie_features.columns if col.startswith("genre_") or col.startswith("tag_")]
        final_features.extend(categorical_cols)

        # Create final feature DataFrame
        self.movie_features = movie_features[final_features].copy()

        print(f"Created movie feature matrix with {len(self.movie_features)} rows and {len(final_features)} columns")
        return self.movie_features

    def process_user_features(self):
        """Process all user features and create encoding"""
        print("Processing user features...")

        if self.users_df is None or self.users_df.empty:
            print("No user data available")
            return None

        # Create a copy to avoid modifying original
        user_features = self.users_df.copy()

        # --- Process gender with one-hot encoding ---
        if "gender" in user_features.columns:
            print("One-hot encoding gender...")
            gender_encoder = OneHotEncoder(sparse_output=False)
            gender_encoded = gender_encoder.fit_transform(user_features[["gender"]].fillna("unknown"))

            gender_cols = [f"gender_{val}" for val in gender_encoder.categories_[0]]
            gender_df = pd.DataFrame(gender_encoded, columns=gender_cols)

            # Add encoded columns to user features
            for col in gender_cols:
                user_features[col] = gender_df[col].values

        # --- Process age into bins and one-hot encode ---
        if "age" in user_features.columns:
            print("Binning and encoding age...")

            # Define age bins and labels
            bins = [0, 18, 25, 35, 45, 55, 65, 100]
            labels = ["<18", "18-24", "25-34", "35-44", "45-54", "55-64", "65+"]

            # Convert to numeric and bin
            user_features["age"] = pd.to_numeric(user_features["age"], errors="coerce")
            user_features["age_group"] = pd.cut(
                user_features["age"], bins=bins, labels=labels, right=False
            )

            # One-hot encode age groups
            for label in labels:
                user_features[f"age_group_{label}"] = (user_features["age_group"] == label).astype(int)

        # --- Process occupation with one-hot encoding ---
        if "occupation" in user_features.columns:
            print("One-hot encoding occupation...")
            occupation_encoder = OneHotEncoder(sparse_output=False)
            occupation_encoded = occupation_encoder.fit_transform(user_features[["occupation"]].fillna("unknown"))

            occupation_cols = [f"occupation_{val}" for val in occupation_encoder.categories_[0]]
            occupation_df = pd.DataFrame(occupation_encoded, columns=occupation_cols)

            # Add encoded columns to user features
            for col in occupation_cols:
                user_features[col] = occupation_df[col].values

        # --- Add user statistics from ratings ---
        if self.ratings_df is not None and not self.ratings_df.empty:
            print("Adding user statistics from ratings...")

            # Compute various statistics per user
            user_stats = self.ratings_df.groupby("userId").agg({
                "rating": ["count", "mean", "std", "min", "max"],
                "movieId": "nunique"  # Number of unique movies rated
            }).reset_index()

            # Flatten column names
            user_stats.columns = [
                "userId", "rating_count", "rating_mean", "rating_std",
                "rating_min", "rating_max", "unique_movies_rated"
            ]

            # Fill NaN in std (happens when user has only one rating)
            user_stats["rating_std"] = user_stats["rating_std"].fillna(0)

            # Merge with user features
            user_features = pd.merge(user_features, user_stats, on="userId", how="left")

            # Fill missing statistics with zeros or medians
            for col in ["rating_count", "unique_movies_rated"]:
                user_features[col] = user_features[col].fillna(0)

            for col in ["rating_mean", "rating_std", "rating_min", "rating_max"]:
                median_val = user_features[col].median()
                user_features[col] = user_features[col].fillna(median_val)

        # --- Select final features ---
        # Start with ID column
        final_features = ["userId"]

        # Add all feature columns except the original categorical columns and intermediate columns
        exclude_cols = ["gender", "age", "occupation", "age_group"]
        feature_cols = [col for col in user_features.columns
                      if col not in exclude_cols and col != "userId"]

        final_features.extend(feature_cols)

        # Create final feature DataFrame
        self.user_features = user_features[final_features].copy()

        print(f"Created user feature matrix with {len(self.user_features)} rows and {len(final_features)} columns")
        return self.user_features

    def normalize_features(self, features_df, id_col):
        """Normalize numerical features to 0-1 range"""
        print(f"Normalizing {features_df.shape[1]-1} features...")

        # Create a copy to avoid modifying original
        normalized_df = features_df.copy()

        # Apply min-max scaling to each numerical column except ID column
        for col in normalized_df.columns:
            # Skip ID column and already normalized columns (one-hot encoded)
            if col == id_col or normalized_df[col].nunique() <= 2:
                continue

            # Apply min-max scaling
            min_val = normalized_df[col].min()
            max_val = normalized_df[col].max()

            # Check if there's a range to normalize
            if max_val > min_val:
                normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)

        return normalized_df

    def create_mlp_autoencoder(self, input_dim, embedding_dim):
        """Create an autoencoder model for dimensionality reduction"""
        class Autoencoder(nn.Module):
            def __init__(self, input_size, embedding_size):
                super(Autoencoder, self).__init__()

                # Calculate intermediate dimensions
                hidden_dim = (input_size + embedding_size) // 2

                # Encoder network
                self.encoder = nn.Sequential(
                    nn.Linear(input_size, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, embedding_size),
                    nn.ReLU()
                )

                # Decoder network
                self.decoder = nn.Sequential(
                    nn.Linear(embedding_size, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, input_size),
                    nn.Sigmoid()  # Output between 0-1 for normalized features
                )

            def forward(self, x):
                # Encode input
                embedding = self.encoder(x)
                # Decode embedding
                reconstructed = self.decoder(embedding)
                return embedding, reconstructed

        return Autoencoder(input_dim, embedding_dim)

    def train_autoencoder(self, features_df, id_col, embedding_dim, model_name):
        """
        Train an autoencoder on feature data and return the embeddings

        Args:
            features_df: DataFrame with features
            id_col: Name of the ID column
            embedding_dim: Dimension of the embedding
            model_name: Name for the model ('movie' or 'user')

        Returns:
            DataFrame with IDs and embeddings
        """
        print(f"\nTraining {model_name} autoencoder ({embedding_dim}-D)")

        # Create a copy without ID column for training
        features = features_df.drop(columns=[id_col]).copy()
        input_dim = features.shape[1]

        # Convert to tensor
        features_tensor = torch.FloatTensor(features.values)
        dataset = TensorDataset(features_tensor, features_tensor)  # Input = target for autoencoder
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)

        # Create model
        model = self.create_mlp_autoencoder(input_dim, embedding_dim).to(self.device)

        # Loss and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)

        # Training loop
        for epoch in tqdm(range(self.epochs), desc=f"Training {model_name} embeddings"):
            total_loss = 0

            for batch_x, _ in dataloader:
                batch_x = batch_x.to(self.device)

                # Forward pass
                _, reconstructed = model(batch_x)

                # Compute loss
                loss = criterion(reconstructed, batch_x)

                # Backward pass and optimize
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            # Print progress every 10 epochs
            avg_loss = total_loss / len(dataloader)
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.6f}")

        print(f"Finished training {model_name} autoencoder")

        # Generate embeddings for all items
        model.eval()
        with torch.no_grad():
            embeddings, _ = model(features_tensor.to(self.device))
            embeddings = embeddings.cpu().numpy()

        # Create DataFrame with embeddings
        cols = [f"{model_name}_embed_{i}" for i in range(embedding_dim)]
        embeddings_df = pd.DataFrame(embeddings, columns=cols)
        embeddings_df[id_col] = features_df[id_col].values

        return embeddings_df

    def create_structured_metadata_vectors(self):
        """Main method to create the structured metadata vectors"""
        print("\n=== Processing Structured Metadata & User Features ===")

        # Step 1: Load data
        if not self.load_data():
            print("Failed to load required data")
            return False

        # Step 2: Process movie features
        movie_features = self.process_movie_features()

        # Step 3: Process user features
        user_features = self.process_user_features()

        # Step 4: Normalize features
        movie_features_normalized = self.normalize_features(movie_features, "movieId")
        if user_features is not None:
            user_features_normalized = self.normalize_features(user_features, "userId")

        # Step 5: Create MLP projections (128-D movie & 32-D user vectors)
        print("\nCreating MLP projections for structured features...")

        # Create movie embeddings (128-D)
        movie_embeddings = self.train_autoencoder(
            movie_features_normalized, "movieId", self.movie_embedding_dim, "movie"
        )

        # Create user embeddings (32-D) if user features are available
        user_embeddings = None
        if user_features is not None:
            user_embeddings = self.train_autoencoder(
                user_features_normalized, "userId", self.user_embedding_dim, "user"
            )

        # Step 6: Save results
        output_dir = os.path.join(self.data_dir, "processed")
        os.makedirs(output_dir, exist_ok=True)

        # Save processed features
        movie_features.to_csv(os.path.join(output_dir, "movie_structured_features.csv"), index=False)
        if user_features is not None:
            user_features.to_csv(os.path.join(output_dir, "user_structured_features.csv"), index=False)

        # Save embeddings
        movie_embeddings.to_csv(os.path.join(output_dir, "movie_structured_embeddings.csv"), index=False)
        if user_embeddings is not None:
            user_embeddings.to_csv(os.path.join(output_dir, "user_structured_embeddings.csv"), index=False)

        print(f"\nAll structured metadata processed and saved to {output_dir}")
        print(f"- Movie embeddings: {self.movie_embedding_dim}-D for {len(movie_embeddings)} movies")
        if user_embeddings is not None:
            print(f"- User embeddings: {self.user_embedding_dim}-D for {len(user_embeddings)} users")

        return {
            "movie_features": movie_features,
            "user_features": user_features,
            "movie_embeddings": movie_embeddings,
            "user_embeddings": user_embeddings
        }


def main():
    """Main function to run the structured metadata processing"""
    print("=== Structured Metadata & User Feature Processing ===")

    # Check for GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Create processor and generate vectors
    processor = StructuredDataProcessor(data_dir="../")
    results = processor.create_structured_metadata_vectors()

    if results:
        # Show sample of movie embeddings
        movie_embed_cols = [col for col in results["movie_embeddings"].columns if "embed" in col]
        print("\nSample movie structured embeddings (first 5 dimensions):")
        sample_cols = ["movieId"] + movie_embed_cols[:5]
        print(results["movie_embeddings"][sample_cols].head())

        if results["user_embeddings"] is not None:
            # Show sample of user embeddings
            user_embed_cols = [col for col in results["user_embeddings"].columns if "embed" in col]
            print("\nSample user structured embeddings (first 5 dimensions):")
            sample_cols = ["userId"] + user_embed_cols[:5]
            print(results["user_embeddings"][sample_cols].head())
    else:
        print("Failed to generate structured metadata vectors")


if __name__ == "__main__":
    main()

import os
import torch
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from transformers import ViTFeatureExtractor, ViTModel
from torch import nn
import warnings
import glob

class VisualFeatureExtractor:
    """
    Extract and aggregate visual features from movie still images using ViT
    Inputs: 5 representative still-frames + 1 official poster
    Encoder: ViT → 768-D embeddings per image
    Aggregation: Transformer self-attention aggregator → single 768-D visual vector
    """
    def __init__(self, model_name="google/vit-base-patch16-224"):
        """Initialize the ViT feature extractor and model"""
        print(f"Initializing ViT feature extractor using {model_name}...")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        self.feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
        self.model = ViTModel.from_pretrained(model_name).to(self.device)
        self.model.eval()

        # Self-attention aggregator
        self.aggregator = SelfAttentionAggregator(embedding_dim=768).to(self.device)

    def extract_image_features(self, image_path):
        """Extract 768-D features from a single image using ViT"""
        try:
            image = Image.open(image_path).convert('RGB')
            inputs = self.feature_extractor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                # Get the [CLS] token embedding which represents the entire image
                features = outputs.last_hidden_state[:, 0, :].cpu().numpy()

            return features.flatten()
        except Exception as e:
            print(f"Error extracting features from {image_path}: {e}")
            return np.zeros(768)  # Return zeros if there's an error

    def extract_and_aggregate_features(self, image_paths):
        """
        Extract features from multiple images and aggregate them using self-attention

        Args:
            image_paths: List of paths to movie still images and poster

        Returns:
            768-D aggregated feature vector
        """
        if not image_paths:
            print("No images provided for feature extraction")
            return np.zeros(768)

        # Extract features from each image
        individual_features = []
        for path in image_paths:
            if os.path.exists(path):
                features = self.extract_image_features(path)
                individual_features.append(features)
            else:
                print(f"Warning: Image not found: {path}")

        if not individual_features:
            print("No valid images found for feature extraction")
            return np.zeros(768)

        # Convert to tensor for aggregation
        features_tensor = torch.tensor(np.array(individual_features), dtype=torch.float32).to(self.device)

        # Apply self-attention aggregation
        with torch.no_grad():
            aggregated_features = self.aggregator(features_tensor).cpu().numpy()

        return aggregated_features.flatten()

class SelfAttentionAggregator(nn.Module):
    """
    Self-attention mechanism to aggregate multiple image embeddings into a single vector
    """
    def __init__(self, embedding_dim=768, num_heads=8):
        super(SelfAttentionAggregator, self).__init__()
        self.embedding_dim = embedding_dim

        # Multi-head self-attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=embedding_dim,
            num_heads=num_heads,
            batch_first=True
        )

        # Feed-forward network for final projection
        self.ffn = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.GELU(),
            nn.Linear(embedding_dim, embedding_dim)
        )

        # Layer normalization
        self.norm1 = nn.LayerNorm(embedding_dim)
        self.norm2 = nn.LayerNorm(embedding_dim)

    def forward(self, x):
        """
        Args:
            x: Tensor of shape [num_images, embedding_dim]
               where num_images is the number of still frames + poster

        Returns:
            Tensor of shape [1, embedding_dim] representing the aggregated visual features
        """
        # Self-attention with residual connection
        attn_output, _ = self.self_attention(x, x, x)
        x = x + attn_output
        x = self.norm1(x)

        # Feed-forward with residual
        ffn_output = self.ffn(x)
        x = x + ffn_output
        x = self.norm2(x)

        # Average pooling to get a single vector
        return torch.mean(x, dim=0, keepdim=True)

def process_movie_visual_features(movie_stills_dir, output_file='movie_visual_features.csv'):
    """
    Process all movies in the representative_stills directory and extract ViT features

    Args:
        movie_stills_dir: Directory containing subdirectories with movie stills
        output_file: CSV file to save the extracted features
    """
    print(f"Processing visual features for movies in {movie_stills_dir}...")

    # Initialize the feature extractor
    extractor = VisualFeatureExtractor()

    # Get a list of all movie directories
    movie_dirs = [d for d in glob.glob(os.path.join(movie_stills_dir, "*")) if os.path.isdir(d)]
    print(f"Found {len(movie_dirs)} movie directories")

    # Results to store movie IDs and their visual features
    results = []

    for movie_dir in tqdm(movie_dirs, desc="Processing movies"):
        try:
            # Extract movie ID from directory name
            movie_id = os.path.basename(movie_dir).split('_')[-1]  # Assumes format like "01_tt0111161"
            if not movie_id.startswith('tt'):
                movie_id = 'tt' + movie_id

            # Find all image files in the directory
            image_files = glob.glob(os.path.join(movie_dir, "*.jpg"))

            # Check if IMDB poster exists
            imdb_poster = next((f for f in image_files if 'imdb_poster' in f.lower()), None)
            if not imdb_poster:
                warnings.warn(f"No IMDb poster found for {movie_id}. Consider downloading with the digestion tool.")

            # Check for representative stills
            rep_stills = [f for f in image_files if 'representative' in f.lower() or 'backdrop' in f.lower()]

            # Combine poster and stills, with poster first if available
            all_images = []
            if imdb_poster:
                all_images.append(imdb_poster)
            all_images.extend(rep_stills)

            # Limit to 6 images (1 poster + 5 stills) if we have more
            all_images = all_images[:6]

            if len(all_images) == 0:
                print(f"Warning: No images found for {movie_id}")
                continue

            print(f"Processing {len(all_images)} images for movie {movie_id}")

            # Extract and aggregate features
            visual_features = extractor.extract_and_aggregate_features(all_images)

            # Store results
            feature_dict = {'movie_id': movie_id}
            for i, feat in enumerate(visual_features):
                feature_dict[f'vit_feature_{i+1}'] = feat

            results.append(feature_dict)

        except Exception as e:
            print(f"Error processing {movie_dir}: {e}")

    # Convert results to DataFrame and save
    if results:
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False)
        print(f"Saved visual features for {len(df)} movies to {output_file}")
        return df
    else:
        print("No features were extracted")
        return None

def check_missing_posters(data_dir):
    """Check for movies that are missing IMDb posters"""
    movie_dirs = [d for d in glob.glob(os.path.join(data_dir, "*")) if os.path.isdir(d)]

    missing_posters = []
    for movie_dir in movie_dirs:
        movie_id = os.path.basename(movie_dir).split('_')[-1]
        imdb_poster = os.path.join(movie_dir, "imdb_poster.jpg")

        if not os.path.exists(imdb_poster):
            missing_posters.append(movie_id)

    print(f"Found {len(missing_posters)} movies without IMDb posters")
    return missing_posters

if __name__ == "__main__":
    # Directory containing representative stills for each movie
    REPRESENTATIVE_STILLS_DIR = "representative_stills"

    # Check if we have any missing posters
    missing = check_missing_posters(REPRESENTATIVE_STILLS_DIR)
    if missing:
        print("The following movies are missing IMDb posters:")
        for movie_id in missing[:10]:  # Show first 10
            print(f"  - {movie_id}")
        if len(missing) > 10:
            print(f"  ... and {len(missing) - 10} more")

        print("\nConsider downloading posters using the IMDb digestion tool")

    # Process all movies to extract visual features
    visual_features_df = process_movie_visual_features(REPRESENTATIVE_STILLS_DIR)

    # Display sample of the results
    if visual_features_df is not None:
        print("\nSample of extracted visual features:")
        print(visual_features_df.iloc[:3, :10])  # Show first 3 rows, first 10 columns