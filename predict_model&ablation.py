# -*- coding: utf-8 -*-
"""predict_model&ablation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SxvSTHewX_gfMm2YTuEKLQXYAdB-5Tun
"""

import os
import pandas as pd
import numpy as np
import torch
from torch import nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import json
from contextlib import nullcontext

class RatingPredictionModel(nn.Module):
    """
    MLP Regressor for Rating Prediction
    Architecture:
    - Linear(4360 → 1024) + ReLU + Dropout(0.2)
    - Linear(1024 → 256) + ReLU + Dropout(0.2)
    - Linear(256 → 1) → predicted rating ŷ
    """

    def __init__(self, input_dim=4360):
        super(RatingPredictionModel, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(1024, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        return self.model(x).squeeze()


class MovieRatingDataset(Dataset):
    """Dataset for movie rating prediction"""

    def __init__(self, user_item_matrix, user_cols, item_cols, transform=None):
        """
        Args:
            user_item_matrix: DataFrame with user_id, movie_id, rating and features
            user_cols: List of user feature column names
            item_cols: List of movie feature column names
            transform: Optional transform to apply to the data
        """
        self.data = user_item_matrix
        self.user_cols = user_cols
        self.item_cols = item_cols
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        # Get user features
        user_features = row[self.user_cols].values.astype(np.float32)

        # Get item features
        item_features = row[self.item_cols].values.astype(np.float32)

        # Concatenate user and item features
        features = np.concatenate([user_features, item_features])

        # Get rating
        rating = np.array([row['rating']], dtype=np.float32)

        if self.transform:
            features = self.transform(features)

        return {
            'features': torch.FloatTensor(features),
            'rating': torch.FloatTensor(rating)
        }


class RatingPredictor:
    """
    Rating Prediction Pipeline using MLP Regressor

    Model Architecture:
    - Input: Item vector i ∈ ℝ⁴³²⁸ (768 visual + 2304 textual + 128 movie metadata + 1128 genome)
    - User vector u ∈ ℝ³²
    - Fusion: concatenate [u, i] → ℝ⁴³⁶⁰

    MLP Regressor:
    - Linear(4360 → 1024) + ReLU + Dropout(0.2)
    - Linear(1024 → 256) + ReLU + Dropout(0.2)
    - Linear(256 → 1) → predicted rating ŷ

    Training:
    - Loss: Mean Squared Error + L2 regularization
    - Data Splits: 80% train / 10% val / 10% test
    - Learning rate=1e-4, weight_decay=1e-5, Batch size: 512, Epochs: 20
    """

    def __init__(self, data_dir='.', device=None):
        """Initialize the rating predictor"""
        print("Initializing Rating Prediction Model...")

        self.data_dir = data_dir
        self.device = device if device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Hyperparameters
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.batch_size = 512
        self.epochs = 20

        # Expected feature dimensions
        self.visual_dim = 768
        self.textual_dim = 2304
        self.structured_dim = 128
        self.user_dim = 32
        self.genome_dim = 1128

        # Total input dimension
        self.full_input_dim = self.visual_dim + self.textual_dim + self.structured_dim + self.genome_dim + self.user_dim

        # Results storage
        self.results = {}

        # Track the best models
        self.best_models = {}
        self.best_val_rmse = {}

    def load_data(self):
        """Load user-item matrix with fused features"""
        print("Loading user-item matrix with features...")

        try:
            # Path to data
            user_item_path = os.path.join(self.data_dir, 'user_item_matrix.csv')

            # Load data
            user_item_df = pd.read_csv(user_item_path)
            print(f"Loaded {len(user_item_df)} user-item pairs")

            # Get column types
            user_cols = [col for col in user_item_df.columns if 'user_embed' in col]
            print(f"Found {len(user_cols)} user feature columns")

            # Get all feature types
            visual_cols = [col for col in user_item_df.columns if 'visual_feature' in col]
            print(f"Found {len(visual_cols)} visual feature columns")

            textual_cols = [col for col in user_item_df.columns if 'text_feature' in col]
            print(f"Found {len(textual_cols)} textual feature columns")

            structured_cols = [col for col in user_item_df.columns if 'movie_embed' in col]
            print(f"Found {len(structured_cols)} structured feature columns")

            genome_cols = [col for col in user_item_df.columns if 'genome_tag' in col]
            print(f"Found {len(genome_cols)} genome feature columns")

            # Group item columns
            item_cols = visual_cols + textual_cols + structured_cols + genome_cols
            print(f"Using {len(item_cols)} total item features")

            # Verify dimensions
            expected_dims = {
                'visual': self.visual_dim,
                'textual': self.textual_dim,
                'structured': self.structured_dim,
                'genome': self.genome_dim,
                'user': self.user_dim
            }

            actual_dims = {
                'visual': len(visual_cols),
                'textual': len(textual_cols),
                'structured': len(structured_cols),
                'genome': len(genome_cols),
                'user': len(user_cols)
            }

            for feat_type, exp_dim in expected_dims.items():
                act_dim = actual_dims[feat_type]
                if act_dim != exp_dim:
                    print(f"Warning: Expected {exp_dim} {feat_type} dimensions, but found {act_dim}")

            # Filter Top-250 movies (if filter flag is set)
            if len(user_item_df) > 600000:  # Approximate size after filtering
                print("Applying Top-250 filter...")
                # Assuming movies are already filtered earlier in the pipeline

            # Save columns for later use
            self.user_cols = user_cols
            self.item_cols = item_cols
            self.visual_cols = visual_cols
            self.textual_cols = textual_cols
            self.structured_cols = structured_cols
            self.genome_cols = genome_cols

            return user_item_df, user_cols, item_cols

        except Exception as e:
            print(f"Error loading data: {e}")
            return None, [], []

    def prepare_datasets(self, user_item_df, user_cols, item_cols):
        """Prepare train, validation, and test datasets"""
        print("\nPreparing datasets...")

        try:
            # Create dataset
            dataset = MovieRatingDataset(user_item_df, user_cols, item_cols)

            # Define split sizes
            total_size = len(dataset)
            train_size = int(0.8 * total_size)
            val_size = int(0.1 * total_size)
            test_size = total_size - train_size - val_size

            # Split dataset
            train_dataset, val_dataset, test_dataset = random_split(
                dataset, [train_size, val_size, test_size]
            )

            print(f"Created datasets - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")

            # Create data loaders
            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)
            val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)
            test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)

            return train_loader, val_loader, test_loader

        except Exception as e:
            print(f"Error preparing datasets: {e}")
            return None, None, None

    def train_model(self, model, train_loader, val_loader, model_variant="Full Model"):
        """Train the rating prediction model"""
        print(f"\nTraining {model_variant}...")

        # Move model to device
        model = model.to(self.device)

        # Loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)

        # Initialize tracking variables
        train_losses = []
        val_losses = []
        val_rmse = []
        val_mae = []
        best_val_rmse_value = float('inf')
        best_model = None

        # Training loop
        for epoch in range(self.epochs):
            # Training
            model.train()
            total_train_loss = 0

            with tqdm(train_loader, desc=f"Epoch {epoch+1}/{self.epochs}") as pbar:
                for batch in pbar:
                    # Get data
                    features = batch['features'].to(self.device)
                    ratings = batch['rating'].to(self.device)

                    # Forward pass
                    optimizer.zero_grad()
                    outputs = model(features)

                    # Compute loss
                    loss = criterion(outputs, ratings)

                    # Backward pass
                    loss.backward()
                    optimizer.step()

                    # Update statistics
                    total_train_loss += loss.item() * features.size(0)
                    pbar.set_postfix({'train_loss': loss.item()})

            # Calculate average training loss
            avg_train_loss = total_train_loss / len(train_loader.dataset)
            train_losses.append(avg_train_loss)

            # Validation
            model.eval()
            total_val_loss = 0
            all_outputs = []
            all_labels = []

            with torch.no_grad():
                for batch in val_loader:
                    features = batch['features'].to(self.device)
                    ratings = batch['rating'].to(self.device)

                    outputs = model(features)
                    loss = criterion(outputs, ratings)

                    total_val_loss += loss.item() * features.size(0)

                    all_outputs.extend(outputs.cpu().numpy())
                    all_labels.extend(ratings.cpu().numpy())

            # Calculate average validation loss
            avg_val_loss = total_val_loss / len(val_loader.dataset)
            val_losses.append(avg_val_loss)

            # Calculate RMSE and MAE
            current_rmse = np.sqrt(mean_squared_error(all_labels, all_outputs))
            current_mae = mean_absolute_error(all_labels, all_outputs)

            val_rmse.append(current_rmse)
            val_mae.append(current_mae)

            # Print statistics
            print(f"Epoch {epoch+1}/{self.epochs} - Train Loss: {avg_train_loss:.4f}, "
                  f"Val Loss: {avg_val_loss:.4f}, RMSE: {current_rmse:.4f}, MAE: {current_mae:.4f}")

            # Save best model
            if current_rmse < best_val_rmse_value:
                best_val_rmse_value = current_rmse
                best_model = model.state_dict().copy()

        # Save training history
        history = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_rmse': val_rmse,
            'val_mae': val_mae,
            'best_val_rmse': best_val_rmse_value
        }

        # Save best model
        self.best_models[model_variant] = best_model
        self.best_val_rmse[model_variant] = best_val_rmse_value

        print(f"Finished training {model_variant}. Best Val RMSE: {best_val_rmse_value:.4f}")

        return history

    def evaluate_model(self, model, test_loader, model_variant="Full Model"):
        """Evaluate the model on test data"""
        print(f"\nEvaluating {model_variant} on test set...")

        # Load best model weights
        if model_variant in self.best_models:
            model.load_state_dict(self.best_models[model_variant])

        model = model.to(self.device)
        model.eval()

        # Loss function
        criterion = nn.MSELoss()

        # Evaluation
        total_test_loss = 0
        all_outputs = []
        all_labels = []

        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                features = batch['features'].to(self.device)
                ratings = batch['rating'].to(self.device)

                outputs = model(features)
                loss = criterion(outputs, ratings)

                total_test_loss += loss.item() * features.size(0)

                all_outputs.extend(outputs.cpu().numpy())
                all_labels.extend(ratings.cpu().numpy())

        # Calculate metrics
        test_rmse = np.sqrt(mean_squared_error(all_labels, all_outputs))
        test_mae = mean_absolute_error(all_labels, all_outputs)

        # Save results
        self.results[model_variant] = {
            'RMSE': test_rmse,
            'MAE': test_mae
        }

        print(f"{model_variant} - Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}")

        return test_rmse, test_mae

    def run_ablation_studies(self, user_item_df):
        """Run ablation studies by removing different feature types"""
        print("\n=== Running Ablation Studies ===")

        ablation_studies = [
            {
                'name': 'Without Visual Features',
                'remove_cols': self.visual_cols
            },
            {
                'name': 'Without Tag-Genome Features',
                'remove_cols': self.genome_cols
            }
        ]

        for study in ablation_studies:
            print(f"\nPreparing {study['name']} ablation study...")

            # Create new item columns without the specified feature type
            ablation_item_cols = [col for col in self.item_cols if col not in study['remove_cols']]
            print(f"Using {len(ablation_item_cols)} item features after removing {len(study['remove_cols'])} features")

            # Prepare datasets
            ablation_dataset = MovieRatingDataset(user_item_df, self.user_cols, ablation_item_cols)

            # Define split sizes
            total_size = len(ablation_dataset)
            train_size = int(0.8 * total_size)
            val_size = int(0.1 * total_size)
            test_size = total_size - train_size - val_size

            # Split dataset
            ablation_train, ablation_val, ablation_test = random_split(
                ablation_dataset, [train_size, val_size, test_size]
            )

            # Create data loaders
            train_loader = DataLoader(ablation_train, batch_size=self.batch_size, shuffle=True, num_workers=2)
            val_loader = DataLoader(ablation_val, batch_size=self.batch_size, shuffle=False, num_workers=2)
            test_loader = DataLoader(ablation_test, batch_size=self.batch_size, shuffle=False, num_workers=2)

            # Create model with adjusted input dimension
            input_dim = len(self.user_cols) + len(ablation_item_cols)
            model = RatingPredictionModel(input_dim=input_dim)

            # Train and evaluate
            self.train_model(model, train_loader, val_loader, model_variant=study['name'])
            self.evaluate_model(model, test_loader, model_variant=study['name'])

    def visualize_results(self):
        """Visualize comparison of results from different models"""
        print("\nVisualizing results...")

        if not self.results:
            print("No results to visualize")
            return

        # Extract metrics for comparison
        models = list(self.results.keys())
        rmse_values = [self.results[model]['RMSE'] for model in models]
        mae_values = [self.results[model]['MAE'] for model in models]

        # Set up plot
        fig, ax = plt.subplots(1, 2, figsize=(15, 6))

        # RMSE
        ax[0].bar(models, rmse_values, color='skyblue')
        ax[0].set_title('RMSE Comparison')
        ax[0].set_ylabel('RMSE')
        ax[0].grid(axis='y', linestyle='--', alpha=0.7)

        # Add values on top of bars
        for i, v in enumerate(rmse_values):
            ax[0].text(i, v + 0.01, f"{v:.3f}", ha='center')

        # MAE
        ax[1].bar(models, mae_values, color='lightgreen')
        ax[1].set_title('MAE Comparison')
        ax[1].set_ylabel('MAE')
        ax[1].grid(axis='y', linestyle='--', alpha=0.7)

        # Add values on top of bars
        for i, v in enumerate(mae_values):
            ax[1].text(i, v + 0.01, f"{v:.3f}", ha='center')

        # Adjust layout
        plt.tight_layout()
        plt.savefig(os.path.join(self.data_dir, 'model_comparison.png'))
        print(f"Saved comparison visualization to {os.path.join(self.data_dir, 'model_comparison.png')}")

        # Generate table for report
        results_table = pd.DataFrame(self.results).T
        print("\nResults Summary:")
        print(results_table)

        # Save results table
        results_table.to_csv(os.path.join(self.data_dir, 'model_results.csv'))

        return results_table

    def run_pipeline(self):
        """Run the complete rating prediction pipeline"""
        print("=== Starting Rating Prediction Pipeline ===\n")

        # Step 1: Load data
        user_item_df, user_cols, item_cols = self.load_data()
        if user_item_df is None:
            print("Failed to load data")
            return False

        # Step 2: Prepare datasets for full model
        train_loader, val_loader, test_loader = self.prepare_datasets(user_item_df, user_cols, item_cols)
        if train_loader is None:
            print("Failed to prepare datasets")
            return False

        # Step 3: Train and evaluate full model
        input_dim = len(user_cols) + len(item_cols)
        full_model = RatingPredictionModel(input_dim=input_dim)

        self.train_model(full_model, train_loader, val_loader, model_variant="Full Model")
        self.evaluate_model(full_model, test_loader, model_variant="Full Model")

        # Step 4: Run ablation studies
        self.run_ablation_studies(user_item_df)

        # Step 5: Visualize results
        self.visualize_results()

        print("\n=== Rating Prediction Pipeline Completed ===")
        return True


def main():
    """Main function to run the rating prediction pipeline"""
    print("=== Movie Rating Prediction ===")

    # Set data directory
    data_dir = '.'

    # Check for GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Create and run predictor
    predictor = RatingPredictor(data_dir=data_dir, device=device)
    predictor.run_pipeline()


if __name__ == "__main__":
    main()