# -*- coding: utf-8 -*-
"""Merge_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1afQ6FcowCgarTuhDGI5ZbvhoJG5ge5C1
"""

!pip install cinemagoer
!pip install selenium
!pip install undetected_chromedriver
!pip install serpapi

import pandas as pd
import numpy as np
import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
import json
import re
from tqdm import tqdm

# Set paths
DATA_DIR = '..'  # Path adjusted to be one level up

def load_and_merge_data():
    """Load MovieLens data and merge with IMDb/TMDB data if available"""
    print("Loading datasets...")

    # Load MovieLens CSVs
    movies_path = os.path.join(DATA_DIR, 'movies.csv')
    ratings_path = os.path.join(DATA_DIR, 'ratings.csv')
    links_path = os.path.join(DATA_DIR, 'links.csv')
    tags_path = os.path.join(DATA_DIR, 'tags.csv')

    try:
        movies_df = pd.read_csv(movies_path)
        print(f"Loaded {len(movies_df)} movies from MovieLens")
    except Exception as e:
        print(f"Error loading movies.csv: {e}")
        movies_df = pd.DataFrame()

    try:
        ratings_df = pd.read_csv(ratings_path)
        print(f"Loaded {len(ratings_df)} ratings from MovieLens")
    except Exception as e:
        print(f"Error loading ratings.csv: {e}")
        ratings_df = pd.DataFrame()

    try:
        links_df = pd.read_csv(links_path)
        print(f"Loaded {len(links_df)} link records from MovieLens")
    except Exception as e:
        print(f"Error loading links.csv: {e}")
        links_df = pd.DataFrame()

    try:
        tags_df = pd.read_csv(tags_path)
        print(f"Loaded {len(tags_df)} tags from MovieLens")
    except Exception as e:
        print(f"Error loading tags.csv: {e}")
        tags_df = pd.DataFrame()

    # Try to load IMDb/TMDB enriched data if available
    try:
        imdb_tmdb_df = pd.read_csv("top_movies.csv")
        print(f"Loaded {len(imdb_tmdb_df)} movies from IMDb/TMDB dataset")
        has_external_data = True
    except Exception as e:
        print(f"IMDb/TMDB dataset not found or error: {e}")
        imdb_tmdb_df = pd.DataFrame()
        has_external_data = False

    # Merge datasets if we have all necessary components
    if not movies_df.empty and not links_df.empty:
        # Merge movies with links to get IMDb IDs
        merged_df = pd.merge(movies_df, links_df, on='movieId', how='left')

        # Format IMDb IDs properly
        if 'imdbId' in merged_df.columns:
            merged_df['imdbId'] = merged_df['imdbId'].astype(str).str.zfill(7)
            merged_df['imdbId'] = 'tt' + merged_df['imdbId']

        # Try to merge with IMDb/TMDB data if available
        if has_external_data and 'IMDb_ID' in imdb_tmdb_df.columns:
            print("Merging with IMDb/TMDB data...")
            merged_df = pd.merge(merged_df, imdb_tmdb_df, left_on='imdbId', right_on='IMDb_ID', how='left')
            print(f"Dataset after merging has {len(merged_df)} rows")
    else:
        merged_df = movies_df if not movies_df.empty else imdb_tmdb_df

    # Add user data if available
    user_df = None
    try:
        user_path = os.path.join(DATA_DIR, 'users.csv')
        user_df = pd.read_csv(user_path)
        print(f"Loaded {len(user_df)} users")
    except Exception as e:
        print(f"User data not found or error: {e}")
        # Create basic user data from ratings if available
        if not ratings_df.empty:
            print("Creating basic user data from ratings")
            user_df = pd.DataFrame({'userId': ratings_df['userId'].unique()})

    return {
        'movies': merged_df,
        'ratings': ratings_df,
        'tags': tags_df,
        'users': user_df
    }

def extract_year_from_title(movies_df):
    """Extract year from movie titles and create a separate column"""
    if 'title' in movies_df.columns:
        # Extract year using regex pattern (4 digits inside parentheses at the end)
        movies_df['year'] = movies_df['title'].str.extract(r'$(\d{4})$').astype('float')
        # Remove year from title
        movies_df['title_no_year'] = movies_df['title'].str.replace(r'\s*$\d{4}$\s*', '', regex=True)
    return movies_df

def process_movie_features(movies_df, ratings_df=None, tags_df=None):
    """
    Process and create features from movie data including:
    - Extract year from title
    - Process genres with one-hot encoding
    - Calculate movie statistics from ratings
    - Process keywords and tags
    - Add all possible categorical features for one-hot encoding
    """
    print("Processing movie features...")

    # Make a copy to avoid modifying the original
    movies_df = movies_df.copy()

    # Extract year from title if not already available
    if 'year' not in movies_df.columns and 'title' in movies_df.columns:
        movies_df = extract_year_from_title(movies_df)

    # Process genres if available
    if 'genres' in movies_df.columns:
        # Get all unique genres
        all_genres = set()
        for genres in movies_df['genres'].dropna():
            if '|' in str(genres):
                all_genres.update(genres.split('|'))
            else:
                all_genres.add(genres)

        print(f"Found {len(all_genres)} unique genres")

        # Create one-hot encoded columns for each genre
        for genre in all_genres:
            if genre and genre != '(no genres listed)':
                movies_df[f'genre_{genre}'] = movies_df['genres'].apply(
                    lambda x: 1 if isinstance(x, str) and genre in x.split('|') else 0
                )

    # Add rating statistics if ratings data is available
    if ratings_df is not None and not ratings_df.empty:
        print("Calculating movie rating statistics...")
        rating_stats = ratings_df.groupby('movieId').agg({
            'rating': ['count', 'mean', 'std', 'min', 'max']
        }).reset_index()
        rating_stats.columns = ['movieId', 'rating_count', 'rating_mean', 'rating_std', 'rating_min', 'rating_max']

        # Fill NaN values in standard deviation (happens when there's only one rating)
        rating_stats['rating_std'] = rating_stats['rating_std'].fillna(0)

        # Merge with movies dataframe
        movies_df = pd.merge(movies_df, rating_stats, on='movieId', how='left')

    # Process tags if available
    if tags_df is not None and not tags_df.empty:
        print("Processing movie tags...")
        # Aggregate tags for each movie
        movie_tags = tags_df.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()

        # Create TF-IDF features for tags
        tfidf = TfidfVectorizer(max_features=200, stop_words='english', min_df=5)

        if len(movie_tags) > 0:
            tag_features = tfidf.fit_transform(movie_tags['tag'].fillna(''))
            tag_feature_names = [f'tag_{name}' for name in tfidf.get_feature_names_out()]

            tag_df = pd.DataFrame(tag_features.toarray(), columns=tag_feature_names)
            tag_df['movieId'] = movie_tags['movieId'].values

            # Merge tags with movies
            movies_df = pd.merge(movies_df, tag_df, on='movieId', how='left')

    # Process additional TMDB/IMDb features if available
    # Most of these can be one-hot encoded if they are categorical
    categorical_columns = [
        'Kind', 'Color_Info', 'Sound_Mix', 'TMDB_Status', 'TMDB_Original_Language'
    ]

    for col in categorical_columns:
        if col in movies_df.columns:
            # For pipe-separated values, split and create one-hot columns
            if movies_df[col].dtype == 'object' and movies_df[col].str.contains('|', na=False).any():
                unique_values = set()
                for values in movies_df[col].dropna():
                    unique_values.update(values.split('|'))

                for value in unique_values:
                    if value:
                        movies_df[f'{col}_{value}'] = movies_df[col].apply(
                            lambda x: 1 if isinstance(x, str) and value in x.split('|') else 0
                        )
            # For single values, use direct one-hot encoding
            elif not pd.api.types.is_numeric_dtype(movies_df[col]):
                for value in movies_df[col].dropna().unique():
                    if value:
                        movies_df[f'{col}_{value}'] = (movies_df[col] == value).astype(int)

    # Process countries if available
    if 'Countries' in movies_df.columns:
        unique_countries = set()
        for countries in movies_df['Countries'].dropna():
            if isinstance(countries, str) and '|' in countries:
                unique_countries.update(countries.split('|'))
            elif isinstance(countries, str):
                unique_countries.add(countries)

        # Limit to top N countries to avoid too many features
        country_counts = Counter()
        for countries in movies_df['Countries'].dropna():
            if isinstance(countries, str):
                for country in countries.split('|'):
                    country_counts[country] += 1

        top_countries = [country for country, _ in country_counts.most_common(20)]

        for country in top_countries:
            if country:
                movies_df[f'country_{country}'] = movies_df['Countries'].apply(
                    lambda x: 1 if isinstance(x, str) and country in x.split('|') else 0
                )

    # Process languages if available
    if 'Languages' in movies_df.columns:
        top_languages = [lang for lang, _ in Counter([
            l for langs in movies_df['Languages'].dropna()
            for l in (langs.split('|') if isinstance(langs, str) else [])
        ]).most_common(15)]

        for lang in top_languages:
            if lang:
                movies_df[f'language_{lang}'] = movies_df['Languages'].apply(
                    lambda x: 1 if isinstance(x, str) and lang in x.split('|') else 0
                )

    # Process directors, limited to top directors
    if 'Directors' in movies_df.columns:
        top_directors = [dir for dir, _ in Counter([
            d for dirs in movies_df['Directors'].dropna()
            for d in (dirs.split('|') if isinstance(dirs, str) else [])
        ]).most_common(30)]

        for director in top_directors:
            if director:
                movies_df[f'director_{director}'] = movies_df['Directors'].apply(
                    lambda x: 1 if isinstance(x, str) and director in x.split('|') else 0
                )

    # Convert numerical features
    numerical_columns = [
        'Year', 'year', 'Runtime_Minutes', 'Votes', 'Rating', 'Top_250_Rank',
        'TMDB_Runtime', 'TMDB_Vote_Average', 'TMDB_Vote_Count', 'TMDB_Popularity',
        'TMDB_Budget', 'TMDB_Revenue'
    ]

    for col in numerical_columns:
        if col in movies_df.columns:
            # Convert to numeric, coercing errors to NaN
            movies_df[col] = pd.to_numeric(movies_df[col], errors='coerce')

            # Fill missing values with median
            if movies_df[col].isnull().sum() > 0:
                median_val = movies_df[col].median()
                movies_df[col] = movies_df[col].fillna(median_val)

    print(f"Processed features: {movies_df.shape[1]} columns for {len(movies_df)} movies")
    return movies_df

def process_user_features(user_df, ratings_df=None):
    """
    Process user features including:
    - One-hot encoding of gender, age groups, occupation
    - Add user statistics from ratings data
    """
    print("Processing user features...")

    if user_df is None or user_df.empty:
        if ratings_df is not None and not ratings_df.empty:
            # Create basic user dataframe from ratings
            user_df = pd.DataFrame({'userId': ratings_df['userId'].unique()})
            print(f"Created basic user data for {len(user_df)} users from ratings")
        else:
            print("No user data available")
            return None

    # Make a copy to avoid modifying original
    user_features = user_df.copy()

    # Process categorical features if available
    categorical_cols = ['gender', 'occupation']
    for col in categorical_cols:
        if col in user_features.columns:
            for value in user_features[col].dropna().unique():
                if value:
                    user_features[f'{col}_{value}'] = (user_features[col] == value).astype(int)

    # Process age into bins if available
    if 'age' in user_features.columns:
        # Define age bins and labels
        bins = [0, 18, 25, 35, 45, 55, 65, 100]
        labels = ['<18', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']

        # Create age groups
        user_features['age_group'] = pd.cut(
            pd.to_numeric(user_features['age'], errors='coerce'),
            bins=bins,
            labels=labels,
            right=False
        )

        # One-hot encode age groups
        for label in labels:
            user_features[f'age_group_{label}'] = (user_features['age_group'] == label).astype(int)

    # Add user statistics from ratings if available
    if ratings_df is not None and not ratings_df.empty:
        # Calculate various statistics per user
        user_stats = ratings_df.groupby('userId').agg({
            'rating': ['count', 'mean', 'std', 'min', 'max'],
            'movieId': 'nunique'  # Number of unique movies rated
        }).reset_index()

        # Flatten the multi-level columns
        user_stats.columns = ['userId', 'rating_count', 'rating_mean', 'rating_std',
                             'rating_min', 'rating_max', 'unique_movies_rated']

        # Fill NaN values in std (happens when there's only one rating)
        user_stats['rating_std'] = user_stats['rating_std'].fillna(0)

        # Merge with user features
        user_features = pd.merge(user_features, user_stats, on='userId', how='left')

        # Fill missing values
        numerical_cols = ['rating_count', 'rating_mean', 'rating_std', 'rating_min',
                          'rating_max', 'unique_movies_rated']
        for col in numerical_cols:
            if col in user_features.columns:
                user_features[col] = user_features[col].fillna(user_features[col].median())

    print(f"Processed user features: {user_features.shape[1]} columns for {len(user_features)} users")
    return user_features

def create_mlp_embeddings(movie_features_df, user_features_df, movie_dim=128, user_dim=32):
    """
    Create MLP embeddings for movies and users using an autoencoder approach.
    Parameters:
    - movie_features_df: DataFrame with processed movie features
    - user_features_df: DataFrame with processed user features
    - movie_dim: Dimension of movie embedding (default: 128)
    - user_dim: Dimension of user embedding (default: 32)

    Returns:
    - Dictionary with movie and user embeddings
    """
    print(f"\nCreating MLP embeddings: {movie_dim}-D movie & {user_dim}-D user vectors")

    class Autoencoder(nn.Module):
        def __init__(self, input_dim, embedding_dim):
            super(Autoencoder, self).__init__()
            # Encoder layers
            self.encoder = nn.Sequential(
                nn.Linear(input_dim, input_dim // 2),
                nn.ReLU(),
                nn.Linear(input_dim // 2, embedding_dim),
                nn.ReLU()
            )
            # Decoder layers
            self.decoder = nn.Sequential(
                nn.Linear(embedding_dim, input_dim // 2),
                nn.ReLU(),
                nn.Linear(input_dim // 2, input_dim),
                nn.Sigmoid()  # Sigmoid for the output (fits 0-1 range after normalization)
            )

        def forward(self, x):
            # Get embedding
            embedding = self.encoder(x)
            # Reconstruct
            x_reconstructed = self.decoder(embedding)
            return embedding, x_reconstructed

    # Function to train an autoencoder
    def train_autoencoder(features_df, id_col, embedding_dim, model_name, epochs=50, batch_size=64):
        # Remove ID column and get only feature columns
        features = features_df.drop(columns=[id_col])

        # Normalize features to 0-1 range
        scaler = MinMaxScaler()
        features_scaled = scaler.fit_transform(features)

        # Convert to PyTorch tensors
        features_tensor = torch.FloatTensor(features_scaled)
        dataset = TensorDataset(features_tensor, features_tensor)  # Input = Target for autoencoder
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # Initialize the model
        input_dim = features.shape[1]
        model = Autoencoder(input_dim, embedding_dim)

        # Check if GPU is available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        # Loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-3)

        # Training loop
        print(f"Training {model_name} autoencoder ({input_dim} â†’ {embedding_dim})")
        for epoch in tqdm(range(epochs), desc=f"Training {model_name} embedding"):
            total_loss = 0
            for batch_x, _ in dataloader:
                batch_x = batch_x.to(device)

                # Forward pass
                _, reconstructed = model(batch_x)

                # Compute loss
                loss = criterion(reconstructed, batch_x)

                # Backward pass and optimize
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

        # Get embeddings for all items
        model.eval()
        with torch.no_grad():
            embeddings, _ = model(features_tensor.to(device))
            embeddings = embeddings.cpu().numpy()

        # Create DataFrame with embeddings
        embedding_df = pd.DataFrame(
            embeddings,
            columns=[f"{model_name}_embed_{i}" for i in range(embedding_dim)]
        )
        embedding_df[id_col] = features_df[id_col].values

        return embedding_df

    # Process movie embeddings
    if not movie_features_df.empty:
        movie_id_col = 'movieId' if 'movieId' in movie_features_df.columns else movie_features_df.columns[0]
        movie_embeddings = train_autoencoder(movie_features_df, movie_id_col, movie_dim, 'movie')
    else:
        print("No movie features available for embedding")
        movie_embeddings = None

    # Process user embeddings
    if user_features_df is not None and not user_features_df.empty:
        user_id_col = 'userId' if 'userId' in user_features_df.columns else user_features_df.columns[0]
        user_embeddings = train_autoencoder(user_features_df, user_id_col, user_dim, 'user')
    else:
        print("No user features available for embedding")
        user_embeddings = None

    return {
        'movie_embeddings': movie_embeddings,
        'user_embeddings': user_embeddings
    }

def normalize_dataframe(df, exclude_cols=None):
    """Normalize all numerical columns in a dataframe to 0-1 range"""
    if exclude_cols is None:
        exclude_cols = []

    df_normalized = df.copy()

    for col in df.columns:
        if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col]):
            # Skip columns with only 0-1 values (already normalized or binary)
            if df[col].nunique() > 2:
                min_val = df[col].min()
                max_val = df[col].max()

                # Only normalize if there's a range
                if max_val > min_val:
                    df_normalized[col] = (df[col] - min_val) / (max_val - min_val)

    return df_normalized

def process_ml_dataset():
    """Main function to process the MovieLens dataset with IMDb/TMDB enrichment"""

    # Load data
    data = load_and_merge_data()
    movies_df = data['movies']
    ratings_df = data['ratings']
    tags_df = data['tags']
    user_df = data['users']

    if movies_df.empty:
        print("Error: No movie data available")
        return None

    # Process movie features
    movie_features = process_movie_features(movies_df, ratings_df, tags_df)

    # Process user features
    user_features = process_user_features(user_df, ratings_df)

    # Normalize features (important before creating embeddings)
    movie_id_col = 'movieId' if 'movieId' in movie_features.columns else movie_features.columns[0]
    movie_features_normalized = normalize_dataframe(movie_features, exclude_cols=[movie_id_col])

    if user_features is not None:
        user_id_col = 'userId' if 'userId' in user_features.columns else user_features.columns[0]
        user_features_normalized = normalize_dataframe(user_features, exclude_cols=[user_id_col])
    else:
        user_features_normalized = None

    # Create MLP embeddings (128-D movie, 32-D user)
    embeddings = create_mlp_embeddings(movie_features_normalized, user_features_normalized)

    # Save results
    output_dir = os.path.join(DATA_DIR, 'processed')
    os.makedirs(output_dir, exist_ok=True)

    # Save processed features
    movie_features.to_csv(os.path.join(output_dir, 'movie_features.csv'), index=False)
    if user_features is not None:
        user_features.to_csv(os.path.join(output_dir, 'user_features.csv'), index=False)

    # Save embeddings
    if embeddings['movie_embeddings'] is not None:
        embeddings['movie_embeddings'].to_csv(os.path.join(output_dir, 'movie_embeddings.csv'), index=False)

    if embeddings['user_embeddings'] is not None:
        embeddings['user_embeddings'].to_csv(os.path.join(output_dir, 'user_embeddings.csv'), index=False)

    print(f"\nAll processed data saved to {output_dir}")

    # Return the results
    return {
        'movie_features': movie_features,
        'user_features': user_features,
        'movie_embeddings': embeddings['movie_embeddings'],
        'user_embeddings': embeddings['user_embeddings']
    }

if __name__ == "__main__":
    # Check for GPU
    if torch.cuda.is_available():
        print(f"GPU is available: {torch.cuda.get_device_name(0)}")
    else:
        print("GPU not available, using CPU")

    # Process the dataset
    results = process_ml_dataset()

    if results:
        print("\n=== Processing Summary ===")
        print(f"Movie features: {results['movie_features'].shape[1]} columns for {len(results['movie_features'])} movies")

        if results['user_features'] is not None:
            print(f"User features: {results['user_features'].shape[1]} columns for {len(results['user_features'])} users")

        if results['movie_embeddings'] is not None:
            movie_embed_cols = [col for col in results['movie_embeddings'].columns if 'embed' in col]
            print(f"Movie embeddings: {len(movie_embed_cols)}-D for {len(results['movie_embeddings'])} movies")

            # Show sample of first 5 dimensions
            sample_cols = [col for col in results['movie_embeddings'].columns if not 'embed' in col] + movie_embed_cols[:5]
            print("\nSample movie embeddings (first 5 dimensions):")
            print(results['movie_embeddings'][sample_cols].head())

        if results['user_embeddings'] is not None:
            user_embed_cols = [col for col in results['user_embeddings'].columns if 'embed' in col]
            print(f"User embeddings: {len(user_embed_cols)}-D for {len(results['user_embeddings'])} users")

            # Show sample of first 5 dimensions
            sample_cols = [col for col in results['user_embeddings'].columns if not 'embed' in col] + user_embed_cols[:5]
            print("\nSample user embeddings (first 5 dimensions):")
            print(results['user_embeddings'][sample_cols].head())